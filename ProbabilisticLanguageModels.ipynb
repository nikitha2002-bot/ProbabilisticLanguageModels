{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748e87e5",
   "metadata": {},
   "source": [
    "# üß† NLP Foundations & Vector Space Retrieval with Word2Vec  \n",
    "**Course:** PROG8245 ‚Äì Machine Learning Programming  \n",
    "**Workshop:** From Preprocessing to TF-IDF and Word2Vec Extension  \n",
    "**Team:** Team 5  \n",
    "\n",
    "### üë• Team Members:\n",
    "- Mandeep Singh (ID: 8989367)  \n",
    "- Kumari Nikitha Singh (ID: 9053016)  \n",
    "- Krishna (ID: 905861)  \n",
    "\n",
    "---\n",
    "\n",
    "üîç In this extended workshop, we go beyond the classical 6-step NLP pipeline and apply advanced techniques like:\n",
    "- **Word2Vec embeddings**\n",
    "- **Cosine similarity for semantic query matching**\n",
    "- **Bigram language modeling**\n",
    "- **Chain Rule‚Äìbased sentence probability**\n",
    "\n",
    "All models are trained on the **Wikitext-2** dataset, representing real-world Wikipedia-style language data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285d61e",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n",
    "\n",
    "We use the Wikitext-2 (raw v1) dataset from Hugging Face.\n",
    "\n",
    "- Loaded the training split.\n",
    "- Removed empty lines to keep only valid text content.\n",
    "\n",
    "This provides a clean Wikipedia-style corpus for language modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64819fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Wikitext-2 (cleaned Wikipedia text, lightweight)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Extract text content\n",
    "documents = dataset['text']\n",
    "documents = [doc for doc in documents if len(doc.strip()) > 0]  # Remove empty lines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8020bc",
   "metadata": {},
   "source": [
    "## Step 2: Document Viewer Utility\n",
    "\n",
    "To inspect the dataset, we define a helper function:\n",
    "\n",
    "- `view_full_document(documents, doc_id)`: Displays the full content of a document at the given index.\n",
    "- Example usage: `view_full_document(documents, 99)`\n",
    "\n",
    "This is helpful to understand the structure and language style of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345baaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ --- Full Document 99 ---\n",
      "\n",
      " In 1997 , the Museum of Science and Natural History merged with the Little Rock Children 's Museum , which had been located in Union Station , to form the Arkansas Museum of Discovery . The new museum was relocated to a historic building in the Little Rock River Market District . The MacArthur Museum of Arkansas Military History opened on May 19 , 2001 in the Tower Building . The new museum 's goal is to educate and inform visitors about the military history of Arkansas , preserve the Tower Building , honor servicemen and servicewomen of the United States and commemorate the birthplace of Douglas MacArthur . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to print full document by index\n",
    "def view_full_document(documents, doc_id):\n",
    "    if 0 <= doc_id < len(documents):\n",
    "        print(f\"\\nüìÑ --- Full Document {doc_id} ---\\n\")\n",
    "        print(documents[doc_id])\n",
    "    else:\n",
    "        print(\"‚ùå Invalid document ID. Please enter a number between 0 and\", len(documents) - 1)\n",
    "\n",
    "# Example: view full document 7\n",
    "view_full_document(documents, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b7ccac",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization\n",
    "\n",
    "We define a custom tokenizer using regular expressions:\n",
    "\n",
    "- Converts all text to lowercase.\n",
    "- Extracts word characters using the pattern `\\b\\w+\\b` to remove punctuation and symbols.\n",
    "\n",
    "This ensures consistency in word representation before further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e9ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Tokenizer using regex: lowercase and extract only word characters\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a79d8",
   "metadata": {},
   "source": [
    "## Step 4: Apply Tokenization to All Documents\n",
    "\n",
    "- We apply the `tokenize()` function to each document in the corpus.\n",
    "- Then, we flatten all tokenized documents into a single list of tokens (`all_tokens`), which is useful for building models like unigram or Word2Vec.\n",
    "\n",
    "This step gives us a clean tokenized representation of the entire corpus, ready for further normalization or statistical modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ee7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Total documents tokenized: 23767\n",
      "üî¢ Total tokens in corpus: 1750956\n",
      "üß™ Sample tokens: ['valkyria', 'chronicles', 'iii', 'senj≈ç', 'no', 'valkyria', '3', 'unrecorded', 'chronicles', 'japanese', 'Êà¶Â†¥„ÅÆ„É¥„Ç°„É´„Ç≠„É•„É™„Ç¢3', 'lit', 'valkyria', 'of', 'the', 'battlefield', '3', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'is', 'a', 'tactical', 'role']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all documents using the regex-based tokenizer\n",
    "tokenized_docs = [tokenize(doc) for doc in documents]\n",
    "\n",
    "# Flatten all tokens into a single list for model building\n",
    "all_tokens = [token for doc in tokenized_docs for token in doc]\n",
    "\n",
    "# Preview stats\n",
    "print(f\"üìÑ Total documents tokenized: {len(tokenized_docs)}\")\n",
    "print(f\"üî¢ Total tokens in corpus: {len(all_tokens)}\")\n",
    "print(\"üß™ Sample tokens:\", all_tokens[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d0686",
   "metadata": {},
   "source": [
    "## Step 5: Token Normalization (Stopword Removal + Stemming)\n",
    "\n",
    "- We use NLTK's stopword list to remove common non-informative words (like \"the\", \"is\", \"and\").\n",
    "- We apply the **Porter Stemmer** to reduce words to their base/stem form (e.g., \"running\" ‚Üí \"run\").\n",
    "- The `normalize_tokens()` function performs both steps for each document.\n",
    "- The result is a normalized set of tokens that retain semantic meaning while reducing noise and vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d0ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kittu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Total normalized tokens: 1038669\n",
      "üîç Sample normalized tokens: ['valkyria', 'chronicl', 'iii', 'senj≈ç', 'valkyria', '3', 'unrecord', 'chronicl', 'japanes', 'Êà¶Â†¥„ÅÆ„É¥„Ç°„É´„Ç≠„É•„É™„Ç¢3', 'lit', 'valkyria', 'battlefield', '3', 'commonli', 'refer', 'valkyria', 'chronicl', 'iii', 'outsid', 'japan', 'tactic', 'role', 'play', 'video', 'game', 'develop', 'sega', 'media', 'vision']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Normalize a list of tokens\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply normalization to each tokenized document\n",
    "normalized_docs = [normalize_tokens(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Flatten into a single list for modeling\n",
    "normalized_tokens = [token for doc in normalized_docs for token in doc]\n",
    "\n",
    "# Preview\n",
    "print(f\"üßπ Total normalized tokens: {len(normalized_tokens)}\")\n",
    "print(\"üîç Sample normalized tokens:\", normalized_tokens[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5297b8",
   "metadata": {},
   "source": [
    "## Step 6: Unigram and Bigram Language Modeling\n",
    "\n",
    "To estimate sentence probabilities, we build two probabilistic models: **unigram** and **bigram**.\n",
    "\n",
    "### Unigram Model\n",
    "- Counts frequency of each individual word across all documents.\n",
    "- Probability of a word = frequency / total words.\n",
    "\n",
    "### Bigram Model\n",
    "- Tracks frequency of word pairs (bigrams), such as (\"machine\", \"learning\").\n",
    "- Probability of a word given previous = count(w1 ‚Üí w2) / count(w1).\n",
    "- We also apply **Laplace Smoothing** to handle unseen bigrams and prevent zero probability.\n",
    "\n",
    "### Sentence Probability Functions\n",
    "We define multiple scoring methods:\n",
    "- `sentence_score_naive`: Averages individual unigram probabilities (not true probability).\n",
    "- `sentence_prob_chain`: Applies chain rule over unigrams (product of probabilities).\n",
    "- `sentence_prob_bigram`: Computes chained bigram probabilities.\n",
    "- `sentence_prob_bigram_smoothed`: Same as above but with Laplace smoothing.\n",
    "\n",
    "These methods help rank sentences and estimate how likely they are under our language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7fd01975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Sentence: Machine learning improves human decision making\n",
      "‚ùå Na√Øve Avg Word Probability     : 4.735227e-04\n",
      "‚úÖ Chain Rule (Unigram Model)     : 3.441999e-21\n",
      "‚ö†Ô∏è  Bigram Model (No Smoothing)   : 0.000000e+00\n",
      "‚úÖ Bigram Model (Laplace Smoothed): 1.453826e-26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenized_docs = [tokenize(doc) for doc in documents]\n",
    "normalized_docs = [normalize_tokens(doc) for doc in tokenized_docs]\n",
    "\n",
    "# === Step 3: Unigram Model ===\n",
    "all_tokens = [token for doc in normalized_docs for token in doc]\n",
    "unigram_counts = Counter(all_tokens)\n",
    "total_tokens = sum(unigram_counts.values())\n",
    "\n",
    "def unigram_prob(word):\n",
    "    return unigram_counts[word] / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "# === Step 4: Bigram Model ===\n",
    "bigram_model = defaultdict(Counter)\n",
    "for doc in normalized_docs:\n",
    "    for i in range(len(doc) - 1):\n",
    "        w1, w2 = doc[i], doc[i + 1]\n",
    "        bigram_model[w1][w2] += 1\n",
    "\n",
    "# Vocabulary size for smoothing\n",
    "vocab = set(all_tokens)\n",
    "V = len(vocab)\n",
    "\n",
    "def bigram_prob(w1, w2):\n",
    "    count_w1_w2 = bigram_model[w1][w2]\n",
    "    count_w1 = sum(bigram_model[w1].values())\n",
    "    if count_w1 == 0:\n",
    "        return 0.0\n",
    "    return count_w1_w2 / count_w1\n",
    "\n",
    "def bigram_prob_smoothed(w1, w2):\n",
    "    count_w1_w2 = bigram_model[w1][w2]\n",
    "    count_w1 = sum(bigram_model[w1].values())\n",
    "    return (count_w1_w2 + 1) / (count_w1 + V)  # Laplace smoothing\n",
    "\n",
    "# === Step 5: Sentence Scoring Functions ===\n",
    "def sentence_score_naive(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    norm_tokens = normalize_tokens(tokens)\n",
    "    probs = [unigram_prob(word) for word in norm_tokens]\n",
    "    return sum(probs) / len(probs) if probs else 0.0\n",
    "\n",
    "def sentence_prob_chain(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    norm_tokens = normalize_tokens(tokens)\n",
    "    prob = 1.0\n",
    "    for word in norm_tokens:\n",
    "        prob *= unigram_prob(word)\n",
    "    return prob\n",
    "\n",
    "def sentence_prob_bigram(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    norm_tokens = normalize_tokens(tokens)\n",
    "    if not norm_tokens:\n",
    "        return 0.0\n",
    "    prob = unigram_prob(norm_tokens[0])\n",
    "    for i in range(1, len(norm_tokens)):\n",
    "        w1, w2 = norm_tokens[i - 1], norm_tokens[i]\n",
    "        prob *= bigram_prob(w1, w2)\n",
    "    return prob\n",
    "\n",
    "def sentence_prob_bigram_smoothed(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    norm_tokens = normalize_tokens(tokens)\n",
    "    if not norm_tokens:\n",
    "        return 0.0\n",
    "    prob = unigram_prob(norm_tokens[0])\n",
    "    for i in range(1, len(norm_tokens)):\n",
    "        w1, w2 = norm_tokens[i - 1], norm_tokens[i]\n",
    "        prob *= bigram_prob_smoothed(w1, w2)\n",
    "    return prob\n",
    "\n",
    "# === Step 6: Test a Sentence ===\n",
    "sentence = \"Machine learning improves human decision making\"\n",
    "\n",
    "print(\"üß™ Sentence:\", sentence)\n",
    "print(f\"‚ùå Na√Øve Avg Word Probability     : {sentence_score_naive(sentence):.6e}\")\n",
    "print(f\"‚úÖ Chain Rule (Unigram Model)     : {sentence_prob_chain(sentence):.6e}\")\n",
    "print(f\"‚ö†Ô∏è  Bigram Model (No Smoothing)   : {sentence_prob_bigram(sentence):.6e}\")\n",
    "print(f\"‚úÖ Bigram Model (Laplace Smoothed): {sentence_prob_bigram_smoothed(sentence):.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b2f55",
   "metadata": {},
   "source": [
    "## Step 7: Word2Vec Embedding Model\n",
    "\n",
    "We train a Word2Vec model using the preprocessed tokenized documents.\n",
    "\n",
    "### Model Configuration\n",
    "- **Model Type**: Skip-gram (`sg=1`), better for capturing rare word relationships.\n",
    "- **Vector Size**: 100-dimensional embeddings.\n",
    "- **Context Window**: 5 words before/after the target word.\n",
    "- **Min Count**: Ignores words with fewer than 2 occurrences to reduce noise.\n",
    "- **Workers**: Parallel processing using 4 threads.\n",
    "\n",
    "Once trained, this model allows us to retrieve similar words and compute word/document similarities based on vector space proximity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5bb18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kittu\\AppData\\Local\\Temp\\ipykernel_3124\\694430846.py:14: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model on normalized tokenized documents (each doc = list of tokens)\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=normalized_docs,    # list of token lists\n",
    "    vector_size=100,              # embedding dimension\n",
    "    window=5,                     # context window\n",
    "    min_count=2,                  # ignore words with <2 frequency\n",
    "    workers=4,                    # parallel threads\n",
    "    sg=1                          # 1 = skip-gram, 0 = CBOW\n",
    ")\n",
    "\n",
    "# Finalize the model for querying\n",
    "w2v_model.init_sims(replace=True)\n",
    "\n",
    "# Save model (optional)\n",
    "# w2v_model.save(\"wiki_word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a82fe",
   "metadata": {},
   "source": [
    "## Step 8: Vocabulary Inspection\n",
    "\n",
    "After training the Word2Vec model, we inspect the learned vocabulary.\n",
    "\n",
    "The model stores word vectors for all tokens that meet the minimum frequency threshold. Below, we print the top 50 most frequent words based on their index order in the model.\n",
    "\n",
    "This provides a quick glimpse into the dominant terms learned by the model from the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290f88f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Vocabulary Sample:\n",
      "['first', 'one', 'also', 'two', 'time', 'year', 'use', 'game', 'state', 'new', 'includ', 'song', 'would', '1', 'work', 'record', '2', 'three', 'play', 'later', 'may', 'season', 'film', 'follow', 'name', 'citi', 'day', 'unit', 'made', 'releas', '3', 'part', 'second', 'end', 'album', 'number', 'seri', 'call', 'world', 'sever', 'mani', 'war', '000', 'area', 'gener', 'forc', 'howev', 'music', '5', 'south']\n"
     ]
    }
   ],
   "source": [
    "# Show top 50 words in the trained model's vocabulary\n",
    "print(\"üìö Vocabulary Sample:\")\n",
    "print(list(w2v_model.wv.index_to_key[:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a00219",
   "metadata": {},
   "source": [
    "## Step 9: Finding Similar Words Using Word2Vec\n",
    "\n",
    "Using the trained Word2Vec model, we retrieve the most semantically similar words to a given input term ‚Äî in this case, **\"bot\"**.\n",
    "\n",
    "The `most_similar()` function returns words that appear in similar contexts, based on cosine similarity of their vector representations. This demonstrates how Word2Vec captures relationships between words in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b832277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Similar words to :\n",
      "[('sidearm', 0.9824039340019226), ('twofold', 0.9815366864204407), ('superconduct', 0.9801238179206848), ('shortfal', 0.9798026084899902), ('utilitarian', 0.979671061038971), ('mn', 0.9795687198638916), ('unwant', 0.9792008399963379), ('lex', 0.9788586497306824), ('lens', 0.9787328243255615), ('cfd', 0.9784953594207764)]\n"
     ]
    }
   ],
   "source": [
    "print(\"üîó Similar words to :\")\n",
    "print(w2v_model.wv.most_similar(\"bot\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219912f",
   "metadata": {},
   "source": [
    "## Step 10: Semantic Document Search with Cosine Similarity\n",
    "\n",
    "We implement a semantic search system by comparing the cosine similarity between:\n",
    "\n",
    "- The **average vector** of a user query (computed using Word2Vec embeddings), and  \n",
    "- The **precomputed document vectors** (mean of word vectors per document).\n",
    "\n",
    "This enables retrieving the top-k documents that are semantically closest to a given query, even if the exact keywords do not match. The function `query_search()` ranks documents based on similarity scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "043c8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Helper: Compute average vector of a token list\n",
    "def average_vector(tokens):\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    if not vectors:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Precompute vectors for all documents\n",
    "doc_vectors = [average_vector(doc) for doc in normalized_docs]\n",
    "\n",
    "# üîç Query function\n",
    "def query_search(query, top_k=3):\n",
    "    query_tokens = normalize_tokens(tokenize(query))\n",
    "    query_vec = average_vector(query_tokens)\n",
    "    \n",
    "    sims = cosine_similarity([query_vec], doc_vectors)[0]\n",
    "    ranked_indices = np.argsort(sims)[::-1][:top_k]\n",
    "    \n",
    "    print(f\"\\nüîé Top {top_k} documents for query: '{query}'\\n\")\n",
    "    for rank, idx in enumerate(ranked_indices, 1):\n",
    "        print(f\"\\nüìÑ Rank #{rank} ‚Äî Doc {idx} (Score: {sims[idx]:.4f})\")\n",
    "        print(documents[idx][:300], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b786390",
   "metadata": {},
   "source": [
    "## Step 11: Running a Semantic Search Query\n",
    "\n",
    "We now test our semantic retrieval system by running a sample query:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d56ca08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Top 3 documents for query: 'global government and war'\n",
      "\n",
      "\n",
      "üìÑ Rank #1 ‚Äî Doc 22953 (Score: 0.9118)\n",
      " = Mozambican War of Independence = \n",
      " ...\n",
      "\n",
      "\n",
      "üìÑ Rank #2 ‚Äî Doc 535 (Score: 0.8927)\n",
      " During World War II civil aerodromes were taken over for military use , existing military airfields were expanded , and new ones were built . This resulted in a significant inventory of facilities becoming available after the war . Pre @-@ war civil aerodromes , for example Sywell , were returned t ...\n",
      "\n",
      "\n",
      "üìÑ Rank #3 ‚Äî Doc 4666 (Score: 0.8811)\n",
      " After World War I , and with another European war looming , leaders from the historic peace churches met to strategize about how to cooperate with the government to avoid the difficulties of World War I. Holding a common view that any participation in military service was not acceptable , they devi ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_search(\"global government and war\", top_k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6fd90",
   "metadata": {},
   "source": [
    "## Step 12: Predicting the Next Word using Bigram Model\n",
    "\n",
    "This function takes a single input word and returns the most likely next words based on the bigram model.\n",
    "\n",
    "- It looks up all bigrams that start with the given word.\n",
    "- It uses the frequency count to determine the most common next words.\n",
    "- This simulates a basic predictive text or autocomplete behavior.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "006acd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_bigram(prev_word, top_k=5):\n",
    "    if prev_word in bigram_model:\n",
    "        # Get Counter object of next words\n",
    "        next_word_counter = bigram_model[prev_word]\n",
    "        most_common = next_word_counter.most_common(top_k)\n",
    "\n",
    "        print(f\"\\nüîÆ Top {top_k} next words for '{prev_word}':\")\n",
    "        for word, count in most_common:\n",
    "            print(f\"  {prev_word} ‚Üí {word} ({count} times)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No bigram found starting with '{prev_word}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb1443",
   "metadata": {},
   "source": [
    "## Step 13: Bigram Prediction and Word2Vec Similarity Lookup\n",
    "\n",
    "This final step demonstrates two capabilities using the input phrase:\n",
    "\n",
    "1. **Bigram Prediction**:\n",
    "   - It takes the last word of the user phrase.\n",
    "   - It uses the trained bigram model to suggest likely next words.\n",
    "\n",
    "2. **Word2Vec Similar Words**:\n",
    "   - It retrieves semantically similar words to the last word using Word2Vec embeddings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83b98ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÆ Top 5 next words for 'king':\n",
      "  king ‚Üí henri (19 times)\n",
      "  king ‚Üí jame (17 times)\n",
      "  king ‚Üí dublin (15 times)\n",
      "  king ‚Üí georg (13 times)\n",
      "  king ‚Üí jerusalem (11 times)\n",
      "\n",
      "üí° Words similar to 'king':\n",
      "  dublin (score: 0.79)\n",
      "  leinster (score: 0.78)\n",
      "  throne (score: 0.75)\n",
      "  domnal (score: 0.74)\n",
      "  kingship (score: 0.74)\n"
     ]
    }
   ],
   "source": [
    "user_phrase = \"king\"\n",
    "last_word = normalize_tokens(tokenize(user_phrase))[-1]\n",
    "\n",
    "next_word_bigram(last_word)\n",
    "similar_next_word_w2v(last_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586b7cd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a practical NLP pipeline using the Wikitext-2 dataset. The major steps include:\n",
    "\n",
    "- **Loading and Preprocessing**: Cleaned and tokenized raw text, applied stopword removal and stemming.\n",
    "- **Language Modeling**: Built unigram and bigram models to compute word and sentence probabilities with and without smoothing.\n",
    "- **Semantic Representation**: Trained a Word2Vec model on normalized tokens to learn word embeddings.\n",
    "- **Similarity Search**: Used cosine similarity to retrieve the most relevant documents for a given query.\n",
    "- **Next-Word Prediction**: Implemented a bigram-based next-word suggestion tool to model contextual word transitions.\n",
    "\n",
    "Overall, this extended workshop connects classical language models with modern vector-based methods for retrieval and prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9e6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
